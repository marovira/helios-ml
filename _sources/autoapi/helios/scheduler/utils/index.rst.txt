helios.scheduler.utils
======================

.. py:module:: helios.scheduler.utils


Attributes
----------

.. autoapisummary::

   helios.scheduler.utils.SCHEDULER_REGISTRY


Functions
---------

.. autoapisummary::

   helios.scheduler.utils.create_scheduler


Module Contents
---------------

.. py:data:: SCHEDULER_REGISTRY

   Global instance of the registry for schedulers.

   By default, the registry contains the following schedulers:

   .. list-table:: Schedulers
       :header-rows: 1

       * - Scheduler
         - Name
       * - ``torch.optim.lr_scheduler.LambdaLR``
         - LambdaLR
       * - ``torch.optim.lr_scheduler.MultiplicativeLR``
         - MultiplicativeLR
       * - ``torch.optim.lr_scheduler.StepLR``
         - StepLR
       * - ``torch.optim.lr_scheduler.MultiStepLR``
         - MultiStepLR
       * - ``torch.optim.lr_scheduler.ConstantLR``
         - ConstantLR
       * - ``torch.optim.lr_scheduler.LinearLR``
         - LinearLR
       * - ``torch.optim.lr_scheduler.ExponentialLR``
         - ExponentialLR
       * - ``torch.optim.lr_scheduler.PolynomialLR``
         - PolynomialLR
       * - ``torch.optim.lr_scheduler.CosineAnnealingLR``
         - CosineAnnealingLR
       * - ``torch.optim.lr_scheduler.SequentialLR``
         - SequentialLR
       * - ``torch.optim.lr_scheduler.ReduceLROnPlateau``
         - ReduceLROnPlateau
       * - ``torch.optim.lr_scheduler.CyclicLR``
         - CyclicLR
       * - ``torch.optim.lr_scheduler.OneCycleLR``
         - OneCycleLR
       * - ``torch.optim.lr_scheduler.CosineAnnealingWarmRestarts``
         - CosineAnnealingWarmRestarts
       * - :py:class:`helios.scheduler.schedulers.CosineAnnealingRestartLR`
         - CosineAnnealingRestartLR
       * - :py:class:`helios.scheduler.schedulers.MultiStepRestartLR`
         - MultiStepRestartLR

   .. rubric:: Example

   .. code-block:: python

       import helios.optim as hlo
       import helios.scheduler as hls

       # This automatically registers your optimizer.
       @hls.SCHEDULER_REGISTRY.register
       class MyScheduler:
           ...

       # Alternatively you can manually register a scheduler. like this:
       hls.SCHEDULER_REGISTRY.register(MyScheduler)

.. py:function:: create_scheduler(type_name: str, *args: Any, **kwargs: Any) -> torch.optim.lr_scheduler.LRScheduler

   Create the scheduler for the given type.

   :param type_name: the type of the scheduler to create.
   :param args: positional arguments to pass into the scheduler.
   :param kwargs: keyword arguments to pass into the scheduler.

   :returns: The scheduler.


